# IntegratedLLM
A lightweight C++ wrapper for embedding LLMs into standalone executables, exposing C-style APIs callable from Python. Built on top of llama.cpp for efficient CPU inference in secure, offline environments
